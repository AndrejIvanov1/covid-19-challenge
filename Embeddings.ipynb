{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Due to MODULEPATH changes, the following have been reloaded:\r\n",
      "  1) jpeg/9b           3) openblas/0.2.19      5) r/3.5.1\r\n",
      "  2) libpng/1.6.27     4) python_cpu/3.6.4\r\n",
      "\r\n",
      "The following have been reloaded with a version change:\r\n",
      "  1) gcc/4.8.5 => gcc/6.3.0\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!module load gcc/6.3.0 python_gpu/3.7.4 hdf5/1.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "Collecting transformers==2.3.0 (from sentence-transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.0.1 in /cluster/home/cagomes/.local/lib64/python3.6/site-packages (from sentence-transformers) (1.4.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /cluster/apps/python/3.6.4_cpu/lib64/python3.6/site-packages (from sentence-transformers) (1.14.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /cluster/apps/python/3.6.4_cpu/lib64/python3.6/site-packages (from sentence-transformers) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /cluster/home/cagomes/.local/lib64/python3.6/site-packages (from sentence-transformers) (4.45.0)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /cluster/apps/python/3.6.4_cpu/lib64/python3.6/site-packages (from sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied, skipping upgrade: nltk in /cluster/home/cagomes/.local/lib64/python3.6/site-packages (from sentence-transformers) (3.4.5)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /cluster/home/cagomes/.local/lib64/python3.6/site-packages (from transformers==2.3.0->sentence-transformers) (1.12.36)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /cluster/home/cagomes/.local/lib64/python3.6/site-packages (from transformers==2.3.0->sentence-transformers) (2020.4.4)\n",
      "Requirement already satisfied, skipping upgrade: requests in /cluster/apps/python/3.6.4_cpu/lib64/python3.6/site-packages (from transformers==2.3.0->sentence-transformers) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /cluster/home/cagomes/.local/lib64/python3.6/site-packages (from transformers==2.3.0->sentence-transformers) (0.0.38)\n",
      "Requirement already satisfied, skipping upgrade: sentencepiece in /cluster/home/cagomes/.local/lib64/python3.6/site-packages (from transformers==2.3.0->sentence-transformers) (0.1.85)\n",
      "Requirement already satisfied, skipping upgrade: six in /cluster/apps/python/3.6.4_cpu/lib64/python3.6/site-packages (from nltk->sentence-transformers) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /cluster/home/cagomes/.local/lib64/python3.6/site-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /cluster/home/cagomes/.local/lib64/python3.6/site-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.9.5)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.16.0,>=1.15.36 in /cluster/home/cagomes/.local/lib64/python3.6/site-packages (from boto3->transformers==2.3.0->sentence-transformers) (1.15.36)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /cluster/apps/python/3.6.4_cpu/lib64/python3.6/site-packages (from requests->transformers==2.3.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /cluster/apps/python/3.6.4_cpu/lib64/python3.6/site-packages (from requests->transformers==2.3.0->sentence-transformers) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /cluster/apps/python/3.6.4_cpu/lib64/python3.6/site-packages (from requests->transformers==2.3.0->sentence-transformers) (1.25.7)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /cluster/apps/python/3.6.4_cpu/lib64/python3.6/site-packages (from requests->transformers==2.3.0->sentence-transformers) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: click in /cluster/apps/python/3.6.4_cpu/lib64/python3.6/site-packages (from sacremoses->transformers==2.3.0->sentence-transformers) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /cluster/home/cagomes/.local/lib64/python3.6/site-packages (from sacremoses->transformers==2.3.0->sentence-transformers) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /cluster/home/cagomes/.local/lib64/python3.6/site-packages (from botocore<1.16.0,>=1.15.36->boto3->transformers==2.3.0->sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /cluster/apps/python/3.6.4_cpu/lib64/python3.6/site-packages (from botocore<1.16.0,>=1.15.36->boto3->transformers==2.3.0->sentence-transformers) (2.7.2)\n",
      "Installing collected packages: transformers, sentence-transformers\n",
      "  Found existing installation: transformers 2.7.0\n",
      "    Uninstalling transformers-2.7.0:\n",
      "      Successfully uninstalled transformers-2.7.0\n",
      "Successfully installed sentence-transformers-0.2.5.1 transformers-2.3.0\n",
      "\u001b[33mYou are using pip version 18.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U --user sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached https://files.pythonhosted.org/packages/37/ba/dda44bbf35b071441635708a3dd568a5ca6bf29f77389f7c7c6818ae9498/transformers-2.7.0-py3-none-any.whl\n",
      "Collecting tokenizers==0.5.2 (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.7MB 3.5MB/s eta 0:00:01    17% |█████▌                          | 645kB 11.0MB/s eta 0:00:01    34% |███████████                     | 1.3MB 13.3MB/s eta 0:00:01    84% |███████████████████████████▏    | 3.2MB 24.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /cluster/apps/python/3.6.4_cpu/lib64/python3.6/site-packages (from transformers) (1.14.2)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/93/83/71a2ee6158bb9f39a90c0dea1637f81d5eef866e188e1971a1b1ab01a35a/filelock-3.0.12-py3-none-any.whl\n",
      "Collecting dataclasses; python_version < \"3.7\" (from transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/d2/6f02df2616fd4016075f60157c7a0452b38d8f7938ae94343911e0fb0b09/dataclasses-0.7-py3-none-any.whl\n",
      "Collecting boto3 (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/2a/5c/defec5286e293b5dedb757d1a51f3d342e8f098f2dae4134495471330687/boto3-1.12.36-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests in /cluster/apps/python/3.6.4_cpu/lib64/python3.6/site-packages (from transformers) (2.22.0)\n",
      "Collecting sacremoses (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/1c/6359be64e8301b84160f6f6f7936bbfaaa5e9a4eab6cbc681db07600b949/tqdm-4.45.0-py2.py3-none-any.whl (60kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 5.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17 (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/c1/c90beb2dbbfbf19f3634e16a441d5f11fa787bdf0748a35b8b88452c0e78/regex-2020.4.4-cp36-cp36m-manylinux1_x86_64.whl (679kB)\n",
      "\u001b[K    100% |████████████████████████████████| 686kB 7.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.0MB 6.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/a3/43/1e939e1fcd87b827fe192d0c9fc25b48c5b3368902bfb913de7754b0dc03/jmespath-0.9.5-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.4.0,>=0.3.0 (from boto3->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl\n",
      "Collecting botocore<1.16.0,>=1.15.36 (from boto3->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/60/f7/f794559d9ffac2e45a56712efa713472032e9861c82629d0183527e2d771/botocore-1.15.36-py2.py3-none-any.whl\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /cluster/apps/python/3.6.4_cpu/lib64/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /cluster/apps/python/3.6.4_cpu/lib64/python3.6/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /cluster/apps/python/3.6.4_cpu/lib64/python3.6/site-packages (from requests->transformers) (1.25.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /cluster/apps/python/3.6.4_cpu/lib64/python3.6/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: six in /cluster/apps/python/3.6.4_cpu/lib64/python3.6/site-packages (from sacremoses->transformers) (1.11.0)\n",
      "Requirement already satisfied: click in /cluster/apps/python/3.6.4_cpu/lib64/python3.6/site-packages (from sacremoses->transformers) (7.0)\n",
      "Collecting joblib (from sacremoses->transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\n",
      "\u001b[K    100% |████████████████████████████████| 296kB 7.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docutils<0.16,>=0.10 (from botocore<1.16.0,>=1.15.36->boto3->transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl (547kB)\n",
      "\u001b[K    100% |████████████████████████████████| 552kB 6.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /cluster/apps/python/3.6.4_cpu/lib64/python3.6/site-packages (from botocore<1.16.0,>=1.15.36->boto3->transformers) (2.7.2)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Running setup.py bdist_wheel for sacremoses ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /cluster/home/cagomes/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, filelock, dataclasses, jmespath, docutils, botocore, s3transfer, boto3, regex, joblib, tqdm, sacremoses, sentencepiece, transformers\n",
      "\u001b[33m  The script tqdm is installed in '/cluster/home/cagomes/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  The script sacremoses is installed in '/cluster/home/cagomes/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed boto3-1.12.36 botocore-1.15.36 dataclasses-0.7 docutils-0.15.2 filelock-3.0.12 jmespath-0.9.5 joblib-0.14.1 regex-2020.4.4 s3transfer-0.3.3 sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 tqdm-4.45.0 transformers-2.7.0\n",
      "\u001b[33mYou are using pip version 18.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --user transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0406 14:08:41.146720 47003742518720 file_utils.py:35] PyTorch version 1.4.0 available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import islice\n",
    "import os\n",
    "import json\n",
    "from time import time\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.notebook import tnrange, tqdm\n",
    "\n",
    "text_path = 'data/preprocessed_text.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33375"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/preprocessed_text.json', 'r') as f:\n",
    "    articles = json.load(f)\n",
    "len(articles.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0406 14:08:48.613653 47003742518720 tokenization_utils.py:327] Model name 'allenai/scibert_scivocab_uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming 'allenai/scibert_scivocab_uncased' is a path or url to a directory containing tokenizer files.\n",
      "I0406 14:08:48.615148 47003742518720 tokenization_utils.py:359] Didn't find file allenai/scibert_scivocab_uncased/added_tokens.json. We won't load it.\n",
      "I0406 14:08:48.616014 47003742518720 tokenization_utils.py:359] Didn't find file allenai/scibert_scivocab_uncased/special_tokens_map.json. We won't load it.\n",
      "I0406 14:08:48.616814 47003742518720 tokenization_utils.py:359] Didn't find file allenai/scibert_scivocab_uncased/tokenizer_config.json. We won't load it.\n",
      "I0406 14:08:49.133502 47003742518720 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_uncased/vocab.txt from cache at /cluster/home/cagomes/.cache/torch/transformers/e3debd8fbdf40874753724814ee0520f612b577b26c8755bca485103b47cd3bc.60287becc5ab96d85a4bf377eb90feaf3b9c80d3b23e84311dccd3588f56d4fb\n",
      "I0406 14:08:49.135159 47003742518720 tokenization_utils.py:395] loading file None\n",
      "I0406 14:08:49.137100 47003742518720 tokenization_utils.py:395] loading file None\n",
      "I0406 14:08:49.138670 47003742518720 tokenization_utils.py:395] loading file None\n",
      "I0406 14:08:49.627057 47003742518720 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_uncased/config.json from cache at /cluster/home/cagomes/.cache/torch/transformers/199e28e62d2210c23d63625bd9eecc20cf72a156b29e2a540d4933af4f50bda1.79c4dd84b76a6991002b44cd58102c732c37aba834ad6401ddd6a89bd0ed809b\n",
      "I0406 14:08:49.628837 47003742518720 configuration_utils.py:199] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "I0406 14:08:50.113971 47003742518720 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_uncased/pytorch_model.bin from cache at /cluster/home/cagomes/.cache/torch/transformers/a4e19031683f34af5fd1c4cca73a3dbe33f8b9e50ad91ddf12ceac577b93c433.7587182ea55c40bf7fd0961c1176c31fa22558da2bf20c199874fa5a8ecb4613\n",
      "I0406 14:08:51.925858 47003742518720 tokenization_utils.py:327] Model name 'deepset/covid_bert_base' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming 'deepset/covid_bert_base' is a path or url to a directory containing tokenizer files.\n",
      "I0406 14:08:51.927496 47003742518720 tokenization_utils.py:359] Didn't find file deepset/covid_bert_base/added_tokens.json. We won't load it.\n",
      "I0406 14:08:51.928647 47003742518720 tokenization_utils.py:359] Didn't find file deepset/covid_bert_base/special_tokens_map.json. We won't load it.\n",
      "I0406 14:08:51.929744 47003742518720 tokenization_utils.py:359] Didn't find file deepset/covid_bert_base/tokenizer_config.json. We won't load it.\n",
      "I0406 14:08:52.390225 47003742518720 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/covid_bert_base/vocab.txt from cache at /cluster/home/cagomes/.cache/torch/transformers/f3e7dca32d8125fcb98fc2360ceec100a5773a3df6796805ed252e3d17be66f3.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0406 14:08:52.391475 47003742518720 tokenization_utils.py:395] loading file None\n",
      "I0406 14:08:52.392476 47003742518720 tokenization_utils.py:395] loading file None\n",
      "I0406 14:08:52.393395 47003742518720 tokenization_utils.py:395] loading file None\n",
      "I0406 14:08:52.914227 47003742518720 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/covid_bert_base/config.json from cache at /cluster/home/cagomes/.cache/torch/transformers/227dc1f3377ab049cd65704a5ae537377a17e05ce90335665027e69fb4b27964.804ce9fd05d921b5f03e94cad964ec79867412490230d4f5c92b036968c16ef1\n",
      "I0406 14:08:52.916503 47003742518720 configuration_utils.py:199] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"language\": \"english\",\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"name\": \"Bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0406 14:08:53.372868 47003742518720 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/covid_bert_base/pytorch_model.bin from cache at /cluster/home/cagomes/.cache/torch/transformers/5368c59872e76efc668918bcc2eca3b1c4295297965b4aacb13ef3be582e9e56.4f0eb07c9ea40465c550765993aae00095b4ea2010153be5e32dcb19197dce6c\n",
      "I0406 14:08:54.965419 47003742518720 SentenceTransformer.py:29] Load pretrained SentenceTransformer: bert-base-nli-mean-tokens\n",
      "I0406 14:08:54.966964 47003742518720 SentenceTransformer.py:32] Did not find a '/' or '\\' in the name. Assume to download model from server.\n",
      "I0406 14:08:54.969974 47003742518720 SentenceTransformer.py:68] Load SentenceTransformer from folder: /cluster/home/cagomes/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\n",
      "I0406 14:08:55.032689 47003742518720 configuration_utils.py:182] loading configuration file /cluster/home/cagomes/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/config.json\n",
      "I0406 14:08:55.034131 47003742518720 configuration_utils.py:199] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0406 14:08:55.035675 47003742518720 modeling_utils.py:403] loading weights file /cluster/home/cagomes/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/pytorch_model.bin\n",
      "I0406 14:08:56.564816 47003742518720 tokenization_utils.py:327] Model name '/cluster/home/cagomes/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/cluster/home/cagomes/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT' is a path or url to a directory containing tokenizer files.\n",
      "I0406 14:08:56.567535 47003742518720 tokenization_utils.py:359] Didn't find file /cluster/home/cagomes/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/tokenizer_config.json. We won't load it.\n",
      "I0406 14:08:56.568279 47003742518720 tokenization_utils.py:395] loading file /cluster/home/cagomes/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/vocab.txt\n",
      "I0406 14:08:56.568869 47003742518720 tokenization_utils.py:395] loading file /cluster/home/cagomes/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/added_tokens.json\n",
      "I0406 14:08:56.569495 47003742518720 tokenization_utils.py:395] loading file /cluster/home/cagomes/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/special_tokens_map.json\n",
      "I0406 14:08:56.570072 47003742518720 tokenization_utils.py:395] loading file None\n",
      "I0406 14:08:56.622539 47003742518720 SentenceTransformer.py:89] Use pytorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "# original scibert\n",
    "tokenizer_scibert = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model_scibert = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "# bert finetuned on covid\n",
    "tokenizer_covid = AutoTokenizer.from_pretrained('deepset/covid_bert_base')\n",
    "model_covid = AutoModel.from_pretrained('deepset/covid_bert_base')\n",
    "# bert for sentences\n",
    "model_sent = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(first, second):\n",
    "    with torch.no_grad():\n",
    "        numerator = torch.dot(first, second)\n",
    "        denominator = torch.norm(first) * torch.norm(second)\n",
    "        return (numerator / denominator).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(sentence, average=False):\n",
    "    encoded = tokenizer_scibert.encode(sentence, add_special_tokens=True)\n",
    "    with torch.no_grad():\n",
    "         output = model_scibert(torch.tensor([encoded]))\n",
    "    if average:\n",
    "        return output[0].squeeze().mean(axis=0)\n",
    "    return output[0][:, 0, :].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(first, second):\n",
    "    return cosine_similarity(sentence_embedding(first), sentence_embedding(second))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7844191789627075"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_similarity(\"What are the risk factors for the virus?\", \"Fever was one of the symptoms of the virus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate title embeddings OR just load them\n",
    "Generation will take about 30 minutes per model for the full titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop paper titles to the first sentence. Drop those that are still too large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_from_lists(lists, indices):\n",
    "    for index in sorted(indices_to_drop, reverse=True):\n",
    "        for l in lists:\n",
    "            l.pop(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encodings_drop_long(text, tokenizer, max_length=30):\n",
    "    encoded = [tokenizer.encode(title, add_special_tokens=True) for title in text]\n",
    "    padded = []\n",
    "    # get rid of titles longer than 30 tokens\n",
    "    dropped = 0\n",
    "    indices_to_drop = []\n",
    "    for index, s in enumerate(encoded):\n",
    "        if len(s) > max_length:\n",
    "            indices_to_drop.append(index)\n",
    "            dropped += 1\n",
    "            continue\n",
    "        padded.append(s)\n",
    "        for i in range(len(s), max_length):\n",
    "            padded[-1].append(0)\n",
    "    print(\"Dropped {} titles\".format(dropped))\n",
    "    return torch.tensor(padded), indices_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 65 titles\n",
      "Dropped 32 titles\n",
      "Dropped 0 titles\n",
      "Dropped 0 titles\n"
     ]
    }
   ],
   "source": [
    "# select n papers\n",
    "n = 500\n",
    "max_length = 30\n",
    "selection = take(500, articles)\n",
    "selected_papers = {key: articles[key] for key in selection}\n",
    "titles = [paper_json[1]['title'] for paper_json in selected_papers.items()]\n",
    "cropped_titles = []\n",
    "for title in titles:\n",
    "    dot_index = title.find(\".\")\n",
    "    if dot_index == -1:\n",
    "        cropped_titles.append(title)\n",
    "    else:\n",
    "        cropped_titles.append(title[0:dot_index + 1])\n",
    "\n",
    "# first run removes titles that are too long, \n",
    "# second run builds actual embeddings once both tokenizers have removed those that are too long\n",
    "\n",
    "encoded_scibert, indices_to_drop = get_encodings_drop_long(cropped_titles, tokenizer_scibert, max_length = max_length)\n",
    "drop_from_lists([cropped_titles, titles], indices_to_drop)\n",
    "\n",
    "encoded_covid, indices_to_drop = get_encodings_drop_long(cropped_titles, tokenizer_covid, max_length = max_length)\n",
    "drop_from_lists([cropped_titles, titles], indices_to_drop)\n",
    "\n",
    "encoded_scibert, indices_to_drop = get_encodings_drop_long(cropped_titles, tokenizer_scibert, max_length = max_length)\n",
    "drop_from_lists([cropped_titles, titles], indices_to_drop)\n",
    "\n",
    "encoded_covid, indices_to_drop = get_encodings_drop_long(cropped_titles, tokenizer_covid, max_length = max_length)\n",
    "drop_from_lists([cropped_titles, titles], indices_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 51/51 [00:14<00:00,  3.61it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "title_generator = data.DataLoader(encoded_scibert, batch_size=batch_size, num_workers=4)\n",
    "embeddings_scibert = torch.zeros(encoded_scibert.shape[0], 768)\n",
    "embeddings_covid = torch.zeros(encoded_scibert.shape[0], 768)\n",
    "with torch.no_grad():\n",
    "    cur_index = 0\n",
    "    t = tqdm(iter(title_generator), leave=False, total=len(title_generator))\n",
    "    for i, batch in enumerate(t):\n",
    "        cur_index += batch_size\n",
    "        output_scibert = model_scibert(batch)\n",
    "        embeddings_scibert[cur_index - batch_size: cur_index] = output_scibert[0][:, 0, :]\n",
    "\n",
    "title_generator = data.DataLoader(encoded_covid, batch_size=batch_size, num_workers=4)\n",
    "with torch.no_grad():\n",
    "    cur_index = 0\n",
    "    t = tqdm(iter(title_generator), leave=False, total=len(title_generator))\n",
    "    for i, batch in enumerate(t):\n",
    "        cur_index += batch_size\n",
    "        output_covid = model_covid(batch)\n",
    "        embeddings_covid[cur_index - batch_size: cur_index] = output_covid[0][:, 0, :]\n",
    "\n",
    "embeddings_sent = torch.tensor(model_sent.encode(cropped_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(embeddings, \"embeddings.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.load(\"embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_embedding(tokenizer, model, query):\n",
    "    query_enc = tokenizer.encode(query, add_special_tokens=True)\n",
    "    for i in range(len(query_enc), max_length):\n",
    "        query_enc.append(0)\n",
    "    query_enc = torch.tensor([query_enc])\n",
    "    with torch.no_grad():\n",
    "        query_output = model(query_enc)\n",
    "    return query_output[0][:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.60it/s]\n"
     ]
    }
   ],
   "source": [
    "query = \"Spread of covid-19 models\"\n",
    "query_embedding_scibert = get_query_embedding(tokenizer_scibert, model_scibert, query)\n",
    "query_embedding_covid = get_query_embedding(tokenizer_covid, model_covid, query)\n",
    "query_embedding_sent = torch.tensor(model_sent.encode([query]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_n_similar(embeddings, query_embedding, n=20):\n",
    "    similarity = F.cosine_similarity(embeddings, query_embedding)\n",
    "    index_sorted = torch.argsort(similarity, descending=True)\n",
    "    return [titles[i] for i in index_sorted.tolist()[:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adaptive Estimation for Epidemic Renewal and Phylogenetic Skyline Models', 'Transmission potential of COVID-19 in Iran 1', 'BAR scaffolds drive membrane fission by crowding disordered domains', 'Recombinant vector vaccines and within-host evolution 1', 'Estimation of risk factors for COVID-19 mortality -preliminary results', 'Title: Probabilistic reconstruction of measles transmission Authors', 'Epidemic analysis of COVID-19 in China by dynamical modeling', 'Similar ratios of introns to intergenic sequence across animal 1 genomes', '(dry-33 iminli@vip', 'Kin and group selection are both flawed but useful data analysis tools', 'Machine intelligence design of 2019-nCoV drugs', 'A Generalized Discrete Dynamic Model for Human Epidemics', 'TALC: Transcription Aware Long Read Correction', 'Retrospective Analysis of Clinical Features in 101 Death Cases with COVID-19', 'Remote control of neural function by X-ray-induced scintillation', 'Estimating Spot Prevalence of COVID-19 from Daily Death Data in Italy', 'A Toolkit for Rapid Modular Construction of Biological Circuits in Mammalian Cells', 'Cross-sectional Study Affiliations', 'COVID-19 in Japan: What could happen in the future?', 'A simple laboratory parameter facilitates early identification of COVID-19 patients']\n"
     ]
    }
   ],
   "source": [
    "n = 20\n",
    "print(find_top_n_similar(embeddings_scibert, query_embedding_scibert, n=n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Speed and strength of an epidemic intervention', 'Machine intelligence design of 2019-nCoV drugs', 'Cross-sectional Study Affiliations', 'SKEMPI 2.0: An updated benchmark of changes in protein-protein binding energy, kinetics and thermodynamics upon mutation', 'Robitaille et al. 1 DUSP1 regulates apoptosis and cell migration, but not the JIP1-protected cytokine response, during Respiratory Syncytial Virus and Sendai Virus infection', 'Transmission potential of COVID-19 in Iran 1', 'Correcting under-reported COVID-19 case numbers', 'Mycroft-West et al. (2020) Running title: SARS-CoV-2 surface S1 Receptor Binding Domain binds heparin The 2019 coronavirus (SARS-CoV-2) surface protein (Spike) S1 Receptor Binding Domain undergoes conformational change upon heparin binding', 'Evolution and variation of 2019-novel coronavirus', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "print(find_top_n_similar(embeddings_covid, query_embedding_covid, n=n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Transmission interval estimates suggest pre-symptomatic spread of COVID-19', 'Identification of a super-spreading chain of transmission associated with COVID-19', 'Title: COVID-19 Progression Timeline and Effectiveness of Response-to-Spread Interventions across the United States', 'Relations of parameters for describing the epidemic of COVID-19 by the Kermack-McKendrick model', 'Dynamic profile of severe or critical COVID-19 cases', 'Fractal kinetics of COVID-19 pandemic (with update 3/1/20)', 'Association between 2019-nCoV transmission and N95 respirator use', 'Correcting under-reported COVID-19 case numbers', 'Characterizing the transmission and identifying the control strategy for COVID-19 through epidemiological modeling', 'High transmissibility of COVID-19 near symptom onset', 'Estimation of risk factors for COVID-19 mortality -preliminary results', 'Estimates of the severity of COVID-19 disease', 'Transmission potential of COVID-19 in Iran 1', 'A spatial model of CoVID-19 transmission in England and Wales: early spread and peak timing', 'Mutations, Recombination and Insertion in the Evolution of 2019-nCoV', 'Mechanistic-statistical SIR modelling for early estimation of the actual number of cases and mortality rate from COVID-19', 'A model simulation study on effects of intervention measures in Wuhan COVID-19 epidemic', 'Epidemiology of seasonal coronaviruses: Establishing the context for COVID-19 emergence', 'Cross-sectional Study Affiliations', 'Mycroft-West et al. (2020) Running title: SARS-CoV-2 surface S1 Receptor Binding Domain binds heparin The 2019 coronavirus (SARS-CoV-2) surface protein (Spike) S1 Receptor Binding Domain undergoes conformational change upon heparin binding']\n"
     ]
    }
   ],
   "source": [
    "print(find_top_n_similar(embeddings_sent, query_embedding_sent, n=n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_np = embeddings.numpy()\n",
    "centered = embeddings_np - embeddings_np.mean(axis=0)\n",
    "pca = PCA(n_components=50)\n",
    "components = pca.fit_transform(centered)\n",
    "tsne = TSNE()\n",
    "result = tsne.fit_transform(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2ab84e5c22e8>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(result[:, 0], result[:, 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
