{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import islice\n",
    "import os\n",
    "import json\n",
    "from time import time\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.notebook import tnrange, tqdm\n",
    "from utils import *\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "text_path = 'data/preprocessed_text.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33375"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/preprocessed_text.json', 'r') as f:\n",
    "    articles = json.load(f)\n",
    "len(articles.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|█████████████████████████████████████████████████████▋                         | 275M/405M [01:39<00:41, 3.17MB/s]"
     ]
    }
   ],
   "source": [
    "# original scibert\n",
    "tokenizer_scibert = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model_scibert = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "# bert finetuned on covid\n",
    "tokenizer_covid = AutoTokenizer.from_pretrained('deepset/covid_bert_base')\n",
    "model_covid = AutoModel.from_pretrained('deepset/covid_bert_base')\n",
    "# bert for sentences\n",
    "model_sent = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(first, second):\n",
    "    return cosine_similarity(sentence_embedding(tokenizer_scibert, model_scibert, first), sentence_embedding(tokenizer_scibert, model_scibert, second))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentence_similarity(\"What are the risk factors for the virus?\", \"Fever was one of the symptoms of the virus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate title embeddings OR just load them\n",
    "Generation will take about 30 minutes per model for the full titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop paper titles to the first sentence. Drop those that are still too large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select n papers\n",
    "n = 2000\n",
    "max_length = 30\n",
    "selection = take(n, articles)\n",
    "selected_papers = {key: articles[key] for key in selection}\n",
    "titles = [paper_json[1]['title'] for paper_json in selected_papers.items()]\n",
    "cropped_titles = []\n",
    "for title in titles:\n",
    "    dot_index = title.find(\".\")\n",
    "    if dot_index == -1:\n",
    "        cropped_titles.append(title)\n",
    "    else:\n",
    "        cropped_titles.append(title[0:dot_index + 1])\n",
    "\n",
    "# first run removes titles that are too long, \n",
    "# second run builds actual embeddings once both tokenizers have removed those that are too long\n",
    "\n",
    "encoded_scibert, indices_to_drop = get_encodings_drop_long(cropped_titles, tokenizer_scibert, max_length = max_length)\n",
    "drop_from_lists([cropped_titles, titles], indices_to_drop)\n",
    "\n",
    "encoded_covid, indices_to_drop = get_encodings_drop_long(cropped_titles, tokenizer_covid, max_length = max_length)\n",
    "drop_from_lists([cropped_titles, titles], indices_to_drop)\n",
    "\n",
    "encoded_scibert, indices_to_drop = get_encodings_drop_long(cropped_titles, tokenizer_scibert, max_length = max_length)\n",
    "drop_from_lists([cropped_titles, titles], indices_to_drop)\n",
    "\n",
    "encoded_covid, indices_to_drop = get_encodings_drop_long(cropped_titles, tokenizer_covid, max_length = max_length)\n",
    "drop_from_lists([cropped_titles, titles], indices_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "title_generator = data.DataLoader(encoded_scibert, batch_size=batch_size, num_workers=4)\n",
    "embeddings_scibert = torch.zeros(encoded_scibert.shape[0], 768)\n",
    "embeddings_covid = torch.zeros(encoded_scibert.shape[0], 768)\n",
    "with torch.no_grad():\n",
    "    cur_index = 0\n",
    "    t = tqdm(iter(title_generator), leave=False, total=len(title_generator))\n",
    "    for i, batch in enumerate(t):\n",
    "        cur_index += batch_size\n",
    "        output_scibert = model_scibert(batch)\n",
    "        embeddings_scibert[cur_index - batch_size: cur_index] = output_scibert[0][:, 0, :]\n",
    "\n",
    "title_generator = data.DataLoader(encoded_covid, batch_size=batch_size, num_workers=4)\n",
    "with torch.no_grad():\n",
    "    cur_index = 0\n",
    "    t = tqdm(iter(title_generator), leave=False, total=len(title_generator))\n",
    "    for i, batch in enumerate(t):\n",
    "        cur_index += batch_size\n",
    "        output_covid = model_covid(batch)\n",
    "        embeddings_covid[cur_index - batch_size: cur_index] = output_covid[0][:, 0, :]\n",
    "\n",
    "embeddings_sent = torch.tensor(model_sent.encode(cropped_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(embeddings, \"embeddings.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings = torch.load(\"embeddings.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Risk factors for covid-19 death\"\n",
    "query_embedding_scibert = get_query_embedding(tokenizer_scibert, model_scibert, query, max_length=max_length)\n",
    "query_embedding_covid = get_query_embedding(tokenizer_covid, model_covid, query, max_length=max_length)\n",
    "query_embedding_sent = get_query_embedding(None, model_sent, query, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "indices_scibert, titles_scibert = find_top_n_similar(embeddings_scibert, query_embedding_scibert, titles, n=n)\n",
    "titles_scibert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_covid, titles_covid = find_top_n_similar(embeddings_covid, query_embedding_covid, titles, n=n)\n",
    "titles_covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indices_sent, titles_sent = find_top_n_similar(embeddings_sent, query_embedding_sent, titles, n=n)\n",
    "titles_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_scibert = get_tsne_embeddings(embeddings_scibert)\n",
    "tsne_covid = get_tsne_embeddings(embeddings_covid)\n",
    "tsne_sent = get_tsne_embeddings(embeddings_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_query_embeddings(query, n=40):\n",
    "    models = [model_scibert, model_covid, model_sent]\n",
    "    tokenizers = [tokenizer_scibert, tokenizer_covid, None]\n",
    "    embeddings = [embeddings_scibert, embeddings_covid, embeddings_sent]\n",
    "    tsnes = [tsne_scibert, tsne_covid, tsne_sent]\n",
    "    plot_titles = [\"Scibert\", \"Covid\", \"Bert Sentence\"]\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    for index, cur in enumerate(zip(models, tokenizers, embeddings, plot_titles, tsnes)):\n",
    "        query_embedding = get_query_embedding(cur[1], cur[0], query)\n",
    "        similar, _ = find_top_n_similar(cur[2], query_embedding, titles, n=n)\n",
    "        similar = set(similar[:n].tolist())\n",
    "        tsne = cur[4]\n",
    "        for i in range(tsne.shape[0]):\n",
    "            if i in similar:\n",
    "                ax[index].scatter(tsne[i, 0], tsne[i, 1], c='r', s=16)\n",
    "            else:\n",
    "                ax[index].scatter(tsne[i, 0], tsne[i, 1], c='b', s=4)\n",
    "        ax[index].set_title(cur[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_query_embeddings_plotly(query, titles, n=40):\n",
    "    models = [model_scibert, model_covid, model_sent]\n",
    "    tokenizers = [tokenizer_scibert, tokenizer_covid, None]\n",
    "    embeddings = [embeddings_scibert, embeddings_covid, embeddings_sent]\n",
    "    tsnes = [tsne_scibert, tsne_covid, tsne_sent]\n",
    "    plot_titles = [\"Scibert\", \"Covid\", \"Bert Sentence\"]\n",
    "    fig = make_subplots(rows=1, cols=3)\n",
    "    for index, cur in enumerate(zip(models, tokenizers, embeddings, plot_titles, tsnes)):\n",
    "        query_embedding = get_query_embedding(cur[1], cur[0], query)\n",
    "        similar, _ = find_top_n_similar(cur[2], query_embedding, titles, n=n)\n",
    "        similar_set = set(similar[:n].tolist())\n",
    "        tsne = cur[4]\n",
    "        fig.add_trace(go.Scatter(x=tsne[:, 0], y=tsne[:, 1], \\\n",
    "                                 mode=\"markers\", text=titles, \\\n",
    "                                 marker=dict(size=[6 if i in similar_set else 4 for i in range(len(titles))],\\\n",
    "                                             color=['red' if i in similar_set else 'blue' for i in range(len(titles))]))\\\n",
    "                      , 1, index + 1) \n",
    "    fig.update_layout(height=400, width=1000, title_text=\"Visualization of Search Results for '{}'\".format(query))\n",
    "    fig.show()\n",
    "    print(\"Top 10 results:\")\n",
    "    for i in similar[:10]:\n",
    "        print(titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_query_embeddings_plotly(\"Risk factors for covid-19 death\", titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_query_embeddings_plotly(\"Asymptomatic carriers of the virus\", titles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
