{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "In the original dataset from Kaggle, the papers are organized in different directories based on their source biomedical database. In this notebook, we:\n",
    "\n",
    "1) Aggregate all the papers in a single file.\n",
    "\n",
    "2) Select only the textual data (title, abstract and body of paper).\n",
    "\n",
    "3) Perform some pre-processing of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dirs = {\n",
    "    \"biorxiv\": \"data/biorxiv_medrxiv/biorxiv_medrxiv\",\n",
    "    \"comm_use_subset\": \"data/comm_use_subset/comm_use_subset\",\n",
    "    \"custom_license\": \"data/custom_license/custom_license\",\n",
    "    \"noncomm_use_subset\": \"data/noncomm_use_subset/noncomm_use_subset\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Merge all papers into a single json file\n",
    "#### Step 2: Save only text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053 papers from source biorxiv...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d649ab3fa0304ddfb54e7569bf01f658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1053.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9315 papers from source comm_use_subset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41182144c7a34e8bbff83c13bbdc3af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9315.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20657 papers from source custom_license...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed578b09b564bd09fb4b106d9a7f755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20657.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2350 papers from source noncomm_use_subset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40d1b594fcc40ccb93051328253424c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2350.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving json to data/all_text.json\n"
     ]
    }
   ],
   "source": [
    "def extract_abstract(abstract_parts: list) -> str:\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            abstract_parts: a list, the 'abstract' key of the paper's json.\n",
    "        Output:\n",
    "            The full text of the abstract as a string\n",
    "        \n",
    "        The abstract section in the paper's json gives us a list of the ordered parts of the text from the abstract.\n",
    "        This method combines the divided abstract text into one string for the whole abstract.\n",
    "    \"\"\"\n",
    "    \n",
    "    if abstract_parts == []: # some papers are missing abstracts\n",
    "        full_abstract_text = \"\"\n",
    "    else:\n",
    "        full_abstract_text = ' '.join([abstract_part['text'] for abstract_part in abstract_parts])\n",
    "    \n",
    "    return full_abstract_text\n",
    "\n",
    "def extract_body(body_parts: list) -> str:\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            body_parts: a list, the 'body_text' key of the paper's json.\n",
    "        Output:\n",
    "            The full text of the body as a string\n",
    "        \n",
    "        The body section in the paper's json gives us a list of the ordered parts of the text from the body.\n",
    "        This method combines the divided body text into one string for the whole body text.\n",
    "    \"\"\"\n",
    "    if body_parts == []:\n",
    "        full_body_text = \"\"\n",
    "    else:\n",
    "        full_body_text = ' '.join([body_part['text'] for body_part in body_parts])\n",
    "    \n",
    "    return full_body_text\n",
    "        \n",
    "def combine_all_text(save_to='data/all_text.json'):\n",
    "    \"\"\"\n",
    "        Combines the text data from all datasets into a single json object.\n",
    "        The structure of the json is:\n",
    "        {\n",
    "            \"paper_id\": {\n",
    "                \"title\": title of paper,\n",
    "                \"source: database it comes from,\n",
    "                \"abstract\": abstract of paper,\n",
    "                \"body\": full text of the body\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "    filtered_json = {}\n",
    "    for source, data_dir in data_dirs.items():\n",
    "        filenames = os.listdir(data_dir)\n",
    "        print(\"{} papers from source {}...\".format(len(filenames), source))\n",
    "\n",
    "        for i in tqdm(range(len(filenames))):\n",
    "            #if i == 5:\n",
    "               #break\n",
    "\n",
    "            with open(os.path.join(data_dir, filenames[i]), 'r') as f:\n",
    "                paper_json = json.load(f)\n",
    "\n",
    "            text_json = {\n",
    "                \"title\": paper_json['metadata']['title'],\n",
    "                \"source\": source,\n",
    "                \"abstract\": extract_abstract(paper_json['abstract']),\n",
    "                \"body\": extract_body(paper_json['body_text'])\n",
    "            }\n",
    "\n",
    "            paper_id = paper_json['paper_id']\n",
    "            filtered_json[paper_id] = text_json\n",
    "\n",
    "    with open(save_to, 'w') as f:\n",
    "        print(\"Saving json to {}\".format(save_to))\n",
    "        json.dump(filtered_json, f)\n",
    "\n",
    "all_text_file = 'data/all_text.json'\n",
    "combine_all_text(save_to=all_text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Text pre-processing.\n",
    "* Normalize all letters to lowercase.\n",
    "* Remove stopwords.\n",
    "* Remove punctuation.\n",
    "* Remove numeric tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33375 papers found in file\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4977b1e175ef4ffe8695ddf4e863aec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=33375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = string.punctuation\n",
    "\n",
    "def is_number(x: str) -> bool:\n",
    "    \"\"\"\n",
    "        Helper method, check if string is a number(int or float)\n",
    "    \"\"\"\n",
    "    number_symbols = ['.', '+', '-', '/', '\\\\', 'âˆ’']\n",
    "    for symbol in number_symbols:\n",
    "        x = x.replace(symbol, '')\n",
    "        \n",
    "    return x.isdigit()\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    tokens = [t for t in tokens if t not in punctuation]\n",
    "    tokens = [t for t in tokens if not is_number(t)]\n",
    "    \n",
    "    text = ' '.join(tokens)\n",
    "    #print(tokens)\n",
    "    return text\n",
    "\n",
    "def preprocess_json(paper_json: dict) -> dict:\n",
    "    paper_json['abstract'] = preprocess_text(paper_json['abstract'])\n",
    "    paper_json['body'] = preprocess_text(paper_json['body'])\n",
    "    \n",
    "    return paper_json\n",
    "\n",
    "\n",
    "with open('data/all_text.json', 'r') as f:\n",
    "    all_data_json = json.load(f)\n",
    "\n",
    "print(\"{} papers found in file {}\".format(len(all_data_json.keys()), all_text_file))\n",
    "    \n",
    "preprocessed_json = {}\n",
    "for paper_id, paper_json in tqdm(all_data_json.items()):\n",
    "    preprocessed_json[paper_id] = preprocess_json(paper_json)\n",
    "\n",
    "with open('data/preprocessed_text.json', 'w') as f:\n",
    "    all_data_json = json.dump(preprocessed_json, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
